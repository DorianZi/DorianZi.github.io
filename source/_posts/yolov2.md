---
title: YOLOv2
date: 2019-06-18 22:09:20
tags: ["Deep Learning", "Computer Vision"]
categories: Technic
---

YOLOv2论文在[此](https://arxiv.org/pdf/1612.08242.pdf)

事实上，根据论文来看，作者根据不同的改进提出的是两个模型
1）提高YOLO的精度同时保持速度，这是YOLOv2
2）提出了一种分类和检测的联合训练策略，使得模型更强大,可以检测多达9000个类别，这是YOLO9000

我们先来看YOLOv2对于YOLOv1的改进在哪里。
主要有三点：Batch Normalization, High Resolution，Anchor Boxes

# 1）Batch Normalization

Batch Normalization是一个通用的提高模型泛化能力的方法，并非YOLOv2首创。翻译过来称作“批量标准化”。

要理解这个方法，首先我们要理解机器学习/深度学习的一个前提：我们之所以可以用训练数据训练出一个模型，然后用该模型对测试数据进行预测，是因为我们假设了训练数据和测试数据的分布是一致的。

换句话说，我们训练出一个好的网络，如果它有比较好的泛化能力，说明这个网络除了具有强大推理能力，还具有“保持通过它的数据分布不变”的能力。所以说一个网络被训练好，自然而然会具备这种能力，因为那些权重，偏置等参数被调整成了相应的样子。

不过，这样的话，那些权重，偏置等参数因为既要负责推理还要负责保持分布，渐渐地它们需要努力很久才能调整好。这就是我们所说的收敛速度变慢。

为了不让调整过程变慢，我们可以**主动一点，明确一点**：在网络的每一层后面（比如在CNN每个layer后面）加上专门用来保持数据分布不变的layer，这样原先那些权重，偏置参数们只需要专心训练推理能力就好了。老layer和新layer各司其职，收敛迅速，皆大欢喜。

我们主动加的layer就是Batch Normalization。

## 数学上理解

上面是定性描述，那么如何在数学上理解呢？

先看均值为0方差为1的正态分布：
![](/uploads/yolov2_1.png)
图中可看出任何一个符合该分布的数据，它有65%的概率取值在[-1,1]，95%在[-2,2]

再来看Sigmoid激活函数：
![](/uploads/yolov2_2.png)
可以看到函数的梯度在[-2,2]之外渐渐平缓，到[-4,4]之外，梯度近乎消失。

我们当然不希望数据在经过Sigmoid激活之后，梯度消失，导致训练的收敛速度变慢甚至停滞。我们希望数据经过Sigmoid激活之后还能保持不错的梯度，这样梯度下降能更快速。办法很明显：确保数据在经过Sigmoid之前，满足均值为0方差为1的正态分布。如下图所示

![](/uploads/yolov2_3.png)

## Tradeoff

上面的做法可能会引起异议：如果使用Sigmoid的[-4,4]段，甚至[-2,2]段，就相当于在用一个线性函数，而没有用到非线性部分。那激活函数还有什么意义呢？一个充满线性layer的深度网络等于没有深度，因为多个线性layer叠加等于一个线性layer。

于是我们需要对收敛速度和非线性能力做一个tradeoff(事实上这是各类算法经常遇到的，比如速度和精度的tradeoff)。方法是：把数据变换成标准正态分布之后再加一个线性偏移：y=scale\*x+shift，这样变成不太规则的正态分布，再通往激活函数。这样既能避免收敛太慢，也能使用到激活函数的非线性部分。

另外说明一下，上面新加的两个参数scale和shift的值在每个神经元上不一定相同，且也是通过训练在进行调整的。

# 2）High Resolution

我们知道，YOLOv1（以及大多数目前检测算法）为了获得强大的分类能力，在ImageNet数据集上进行了预训练，而ImageNet上的图片尺寸是224\*224，所以预训练出来的模型是针对224\*224分辨率的，而接下来进行迁移学习的Fine-tune时，使用的图片尺寸是448\*448，


# 参考

https://www.cnblogs.com/guoyaohua/p/8724433.html