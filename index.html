<!DOCTYPE html>












  


<html class="theme-next muse use-motion" lang>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">


























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=7.0.1">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.0.1">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.0.1">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.0.1">


  <link rel="mask-icon" href="/images/logo.svg?v=7.0.1" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '7.0.1',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta property="og:type" content="website">
<meta property="og:title" content="Dorian Circle">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Dorian Circle">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Dorian Circle">






  <link rel="canonical" href="http://yoursite.com/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Dorian Circle</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Dorian Circle</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home menu-item-active">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/19/PCA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Dorian Zi">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dorian Circle">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/19/PCA/" class="post-title-link" itemprop="url">PCA</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-03-19 10:53:44 / Modified: 12:03:17" itemprop="dateCreated datePublished" datetime="2019-03-19T10:53:44+08:00">2019-03-19</time>
            

            
              

              
            
          </span>

          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="PCA主成分分析"><a href="#PCA主成分分析" class="headerlink" title="PCA主成分分析"></a>PCA主成分分析</h1><h2 id="降维思想"><a href="#降维思想" class="headerlink" title="降维思想"></a>降维思想</h2><p>PCA即Principal Component Analysis, 主成分分析。主要思想是数据降维。</p>
<p>降维的办法常常是利用现有的n维数据的统计特征，创造更少的m维特征来表示该组数据，实现降维。</p>
<p>有三个数据点在正交基<img src="https://latex.codecogs.com/gif.latex?(\underset{i}{\rightarrow},\underset{j}{\rightarrow})" title="(\underset{i}{\rightarrow},\underset{j}{\rightarrow})" style="display:inline;vertical-align:text-top;">下表示为<img src="https://latex.codecogs.com/gif.latex?(x_{1},y_{1}),(x_{2},y_{2}),(x_{3},y_{3})" title="(x_{1},y_{1}),(x_{2},y_{2}),(x_{3},y_{3})" style="display:inline;vertical-align:text-top;"></p>
<p><img src="https://github.com/DorianZi/algorithm_explained/raw/a31c6088efb30703792fd4fbaa4132985eb10e00/res/pca_2.png" style="display:inline;vertical-align:text-top;"></p>
<p>我们可以用2x3矩阵来表示它：</p>
<p><img src="https://latex.codecogs.com/gif.latex?\begin{bmatrix}&space;x_{1}&space;&&space;x_{2}&&space;x_{3}\\&space;y_{1}&space;&&space;y_{2}&&space;y_{3}&space;\end{bmatrix}" title="\begin{bmatrix} x_{1} & x_{2}& x_{3}\\ y_{1} & y_{2}& y_{3} \end{bmatrix}" style="display:inline;vertical-align:text-top;"></p>
<p>这里我们发现数据的x与y两个维度有着规律：x=2y，则我们可以创造出新的维度<img src="https://latex.codecogs.com/gif.latex?\underset{k}{\rightarrow}" title="\underset{k}{\rightarrow}" style="display:inline;vertical-align:text-top;"> :</p>
<p><img src="https://latex.codecogs.com/gif.latex?2\underset{i}{\rightarrow}&plus;\underset{j}{\rightarrow}=\underset{k}{\rightarrow}" title="2\underset{i}{\rightarrow}+\underset{j}{\rightarrow}=\underset{k}{\rightarrow}" style="display:inline;vertical-align:text-top;"></p>
<p>在新维度下表示数据只需要1x3矩阵：</p>
<p><img src="https://latex.codecogs.com/gif.latex?\begin{bmatrix}&space;x_{1}&space;&&space;x_{2}&&space;x_{3}&space;\end{bmatrix}" title="\begin{bmatrix} x_{1} & x_{2}& x_{3} \end{bmatrix}" style="display:inline;vertical-align:text-top;"></p>
<p>至此，实现了降维。</p>
<h2 id="最大方差"><a href="#最大方差" class="headerlink" title="最大方差"></a>最大方差</h2><p>上面是一种特殊情况，即所有数据点恰好在一个向量方向上。通常情况，我们有很多数据点且并非所有数据都能连成一条线。</p>
<p>这时候我们要找到一个维度（一个向量），让所有数据投影（降维）到该向量上之后，尽可能地分散。因为越分散说明通过该向量降维之后数据得以很好地区分，越紧凑说明该降维导致了越多信息丢失。数学上表达“分散”的方式就是：方差。方差越大，则说明数据越分散</p>
<p>为了计算方差方便，对数据进行去均值（即以各维度的均值所指明的点为中点）。接下来我们要找到一个单位向量（为了计算方便）使得投影后数据最分散，即方差最大：</p>
<p><img src="https://pic3.zhimg.com/v2-89d7327bd92119c2c99357a423d4da26_b.gif" style="display:inline;vertical-align:text-top;"></p>
<p><img src="https://github.com/DorianZi/algorithm_explained/blob/master/res/pca_3.png?raw=true" style="display:inline;vertical-align:text-top;"></p>
<p>设该向量为<img src="https://latex.codecogs.com/gif.latex?v" title="v" style="display:inline;vertical-align:text-top;">, 数据点为向量（以中心点为原点作向量）<img src="https://latex.codecogs.com/gif.latex?x_{1},x_{2},...,x_{n}" title="x_{1},x_{2},...,x_{n}" style="display:inline;vertical-align:text-top;"></p>
<p>通过向量点积求投影长度：<img src="https://latex.codecogs.com/gif.latex?d_{i}=\frac{x_{i}^{T}v}{|v|}" title="d_{i}=\frac{x_{i}^{T}v}{|v|}" style="display:inline;vertical-align:text-top;"></p>
<p>则方差为:</p>
<p><img src="https://latex.codecogs.com/gif.latex?\sigma&space;^{2}=\sum_{i=1}^{n}d_{i}^{2}=\sum_{i=1}^{n}(\frac{x_{i}^{T}v}{|v|})^{2}" title="\sigma ^{2}=\sum_{i=1}^{n}d_{i}^{2}=\sum_{i=1}^{n}(\frac{x_{i}^{T}v}{|v|})^{2}" style="display:inline;vertical-align:text-top;"></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src="https://latex.codecogs.com/gif.latex?=\sum_{i=1}^{n}(\frac{x_{i}^{T}vx_{i}^{T}v}{v^{T}v})" title="=\sum_{i=1}^{n}(\frac{x_{i}^{T}vx_{i}^{T}v}{v^{T}v})" style="display:inline;vertical-align:text-top;"></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src="https://latex.codecogs.com/gif.latex?=\sum_{i=1}^{n}v^{T}x_{i}x_{i}^{T}v" title="=\sum_{i=1}^{n}v^{T}x_{i}x_{i}^{T}v" style="display:inline;vertical-align:text-top;"></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src="https://latex.codecogs.com/gif.latex?=v^{T}(\sum_{i=1}^{n}x_{i}x_{i}^{T})v" title="=v^{T}(\sum_{i=1}^{n}x_{i}x_{i}^{T})v" style="display:inline;vertical-align:text-top;"></p>
<p>设<img src="https://latex.codecogs.com/gif.latex?C=\sum_{i=1}^{n}x_{i}x_{i}^{T}" title="C=\sum_{i=1}^{n}x_{i}x_{i}^{T}" style="display:inline;vertical-align:text-top;">， 则C为协方差矩阵。为什么呢？推导如下——</p>
<p>对于一个有m个n维样本的样本集，我们把它写成行向量形式（只是为了方便观察，写成列向量也可以。并不影响协方差矩阵的shape），即每行为一个样本，每一列为一个维度，且维度分别为D1,D2,…,Dn：</p>
<p><img src="https://latex.codecogs.com/gif.latex?\underset{m\times&space;n}{Data}=&space;\overset{D1&space;\&space;\&space;\&space;D2&space;\&space;\&space;\&space;\&space;...&space;\&space;\&space;\&space;Dn}{&space;\begin{bmatrix}&space;x_{11}&&space;x_{12}&&space;...&space;&&space;x_{1n}\\&space;x_{21}&&space;x_{22}&&space;...&space;&&space;x_{2n}\\&space;&&space;&...&space;&&space;\\&space;x_{m1}&&space;x_{m2}&&space;...&space;&&space;x_{mn}&space;\end{bmatrix}&space;}" title="\underset{m\times n}{Data}= \overset{D1 \ \ \ D2 \ \ \ \ ... \ \ \ Dn}{ \begin{bmatrix} x_{11}& x_{12}& ... & x_{1n}\\ x_{21}& x_{22}& ... & x_{2n}\\ & &... & \\ x_{m1}& x_{m2}& ... & x_{mn} \end{bmatrix} }" style="display:inline;vertical-align:text-top;"></p>
<p>求该样本集的协方差矩阵：</p>
<p><img src="https://latex.codecogs.com/gif.latex?\underset{n\times&space;n}{C}=&space;\begin{bmatrix}&space;Cov(D1,D1)&&space;Cov(D1,D2)&&space;...&space;&&space;Cov(D1,Dn)\\&space;Cov(D2,D1)&&space;Cov(D2,D2)&&space;...&space;&&space;Cov(D2,Dn)\\&space;&&space;&...&space;&&space;\\&space;Cov(Dn,D1)&&space;Cov(Dn,D2)&&space;...&space;&&space;Cov(Dn,Dn)&space;\end{bmatrix}" title="\underset{n\times n}{C}= \begin{bmatrix} Cov(D1,D1)& Cov(D1,D2)& ... & Cov(D1,Dn)\\ Cov(D2,D1)& Cov(D2,D2)& ... & Cov(D2,Dn)\\ & &... & \\ Cov(Dn,D1)& Cov(Dn,D2)& ... & Cov(Dn,Dn) \end{bmatrix}" style="display:inline;vertical-align:text-top;"></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src="https://latex.codecogs.com/gif.latex?=\frac{1}{m-1}&space;\begin{bmatrix}&space;x_{11}x_{11}&plus;x_{21}x_{21}&plus;...&plus;x_{m1}x_{m1}&&space;x_{11}x_{12}&plus;x_{21}x_{22}&plus;...&plus;x_{m1}x_{m2}&&space;...&space;&&space;x_{11}x_{1n}&plus;x_{21}x_{2n}&plus;...&plus;x_{m1}x_{mn}\\&space;x_{12}x_{11}&plus;x_{22}x_{21}&plus;...&plus;x_{m2}x_{m1}&&space;x_{12}x_{12}&plus;x_{22}x_{22}&plus;...&plus;x_{m2}x_{m2}&&space;...&space;&&space;x_{12}x_{1n}&plus;x_{22}x_{2n}&plus;...&plus;x_{m2}x_{mn}\\&space;&&space;&...&space;&&space;\\&space;x_{1n}x_{11}&plus;x_{2n}x_{21}&plus;...&plus;x_{mn}x_{m1}&&space;x_{1n}x_{12}&plus;x_{2n}x_{22}&plus;...&plus;x_{mn}x_{m2}&&space;...&space;&&space;x_{1n}x_{1n}&plus;x_{2n}x_{2n}&plus;...&plus;x_{mn}x_{mn}&space;\end{bmatrix}" title="=\frac{1}{m-1} \begin{bmatrix} x_{11}x_{11}+x_{21}x_{21}+...+x_{m1}x_{m1}& x_{11}x_{12}+x_{21}x_{22}+...+x_{m1}x_{m2}& ... & x_{11}x_{1n}+x_{21}x_{2n}+...+x_{m1}x_{mn}\\ x_{12}x_{11}+x_{22}x_{21}+...+x_{m2}x_{m1}& x_{12}x_{12}+x_{22}x_{22}+...+x_{m2}x_{m2}& ... & x_{12}x_{1n}+x_{22}x_{2n}+...+x_{m2}x_{mn}\\ & &... & \\ x_{1n}x_{11}+x_{2n}x_{21}+...+x_{mn}x_{m1}& x_{1n}x_{12}+x_{2n}x_{22}+...+x_{mn}x_{m2}& ... & x_{1n}x_{1n}+x_{2n}x_{2n}+...+x_{mn}x_{mn} \end{bmatrix}" style="display:inline;vertical-align:text-top;"></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src="https://latex.codecogs.com/gif.latex?=\frac{1}{m-1}\sum_{i=1}^{n}x_{i}x_{i}^{T}" style="display:inline;vertical-align:text-top;"></p>
<p>常数项不重要，将其隐去，则<img src="https://latex.codecogs.com/gif.latex?C=\sum_{i=1}^{n}x_{i}x_{i}^{T}" title="C=\sum_{i=1}^{n}x_{i}x_{i}^{T}" style="display:inline;vertical-align:text-top;"> 推导完毕。</p>
<p>回到上面的方差公式：<img src="https://latex.codecogs.com/gif.latex?\sigma&space;^{2}=v^{T}Cv" title="\sigma ^{2}=v^{T}Cv" style="display:inline;vertical-align:text-top;"></p>
<p>所以接下来我们要求取一个单位向量<img src="https://latex.codecogs.com/gif.latex?v" title="v" style="display:inline;vertical-align:text-top;">（即约束条件：<img src="https://latex.codecogs.com/gif.latex?v^{T}v=1" title="v^{T}v=1" style="display:inline;vertical-align:text-top;">）使得方差<img src="https://latex.codecogs.com/gif.latex?v^{T}Cv" title="v^{T}Cv" style="display:inline;vertical-align:text-top;">最大</p>
<p>采用拉格朗日乘子法,即转换为求如下函数的极值：</p>
<p><img src="https://latex.codecogs.com/gif.latex?f(v,\lambda)=v^{T}Cv-\lambda(v^{T}v-1)" title="f(v,\lambda)=v^{T}Cv-\lambda(v^{T}v-1)" style="display:inline;vertical-align:text-top;"></p>
<p>求极值，则要求偏导数为0：</p>
<p><img src="https://latex.codecogs.com/gif.latex?\begin{cases}&space;\frac{\partial&space;f}{\partial&space;v}=\frac{\partial&space;(v^{T}Cv)}{\partial&space;v}-\lambda\frac{\partial&space;(v^{T}v)}{\partial&space;v}=2Cv-2\lambda&space;v=0\\&space;\frac{\partial&space;f}{\partial&space;\lambda}=v^{T}v-1=0&space;\end{cases}" title="\begin{cases} \frac{\partial f}{\partial v}=\frac{\partial (v^{T}Cv)}{\partial v}-\lambda\frac{\partial (v^{T}v)}{\partial v}=2Cv-2\lambda v=0\\ \frac{\partial f}{\partial \lambda}=v^{T}v-1=0 \end{cases}" style="display:inline;vertical-align:text-top;"></p>
<p>顺便提一下<img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;(v^{T}Cv)}{v}=2Cv" title="\frac{\partial (v^{T}Cv)}{v}=2Cv" style="display:inline;vertical-align:text-top;">的求导过程，利用了协方差矩阵C为对称实矩阵的特性，不然你得不到<img src="https://latex.codecogs.com/gif.latex?2Cv" title="2Cv" style="display:inline;vertical-align:text-top;">这么完美的结果的。</p>
<p>同时<img src="https://latex.codecogs.com/gif.latex?2Cv-2\lambda&space;v=0&space;=>&space;Cv=\lambda&space;v" title="2Cv-2\lambda v=0 => Cv=\lambda v" style="display:inline;vertical-align:text-top;">，恰好是<a href="https://github.com/DorianZi/algorithm_explained/blob/master/matrix_SVD_decomposition.md#%E7%89%B9%E5%BE%81%E5%80%BC%E5%92%8C%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F" target="_blank" rel="noopener">特征向量和特征值</a>的定义！</p>
<p>于是，上面的极值条件转换为：</p>
<h5 id="求协方差矩阵C的单位特征向量"><a href="#求协方差矩阵C的单位特征向量" class="headerlink" title="求协方差矩阵C的单位特征向量"></a>求协方差矩阵C的单位特征向量</h5><p>不过，特征向量和特征值那么多，哪一个才是使得方差最大呢？</p>
<p><img src="https://latex.codecogs.com/gif.latex?\sigma&space;^{2}=v^{T}Cv=v^{T}\lambda&space;v=\lambda&space;v^{T}v=\lambda" title="\sigma ^{2}=v^{T}Cv=v^{T}\lambda v=\lambda v^{T}v=\lambda" style="display:inline;vertical-align:text-top;"></p>
<p>所以，最大特征值对应的特征向量使得方差最大，也就是使得数据降维之后最分散。</p>
<p>不过在实际PCA应用中，一般不会直接降维到只有一个特征向量。降维是为了在容许丢失部分信息的前提下简化数据复杂度，不是为了简化数据复杂度而去一味地降维。于是，我们会从大到小，选则m个最大的特征值对应的特征向量，组成新的m组维度，来表示原始数据。在PCA里称这是m个主元。</p>
<h2 id="协方差矩阵的意义"><a href="#协方差矩阵的意义" class="headerlink" title="协方差矩阵的意义"></a>协方差矩阵的意义</h2><p>以二维为例，在数据图形中理解协方差矩阵：<br><img src="https://github.com/DorianZi/algorithm_explained/blob/master/res/eigenvectors.png?raw=true" style="display:inline;vertical-align:text-top;"><br><img src="https://github.com/DorianZi/algorithm_explained/blob/master/res/eigenvectors_covariance.png?raw=true" style="display:inline;vertical-align:text-top;"></p>
<p>协方差矩阵对角线上为x方向和方差，它表示x轴方向和y轴方向数据的散度。非对角线上为x,y方向相关度：0表示无关，数据在x轴上延伸，在y轴上无变化；正数表示正相关，数据在x轴正向延伸，在y轴也正向延伸，且越大表示相关性越大；负数表示负相关，数据在x轴正向延伸，就在y轴负向延伸。</p>
<p>在特征向量方向重新表述上面的散度关系，同时利用上面的结论<img src="https://latex.codecogs.com/gif.latex?\sigma&space;^{2}=\lambda" title="\sigma ^{2}==\lambda" style="display:inline;vertical-align:text-top;">，我们得到以下结论：特征向量方向的数据方差大小为对应的特征值；两个特征向量方向数据的协方差为0</p>
<p>我们继续在数学上解释以上结论——</p>
<p>首先找到协方差矩阵的特征向量和特征值，也就是把原矩阵对角化为： <img src="https://latex.codecogs.com/gif.latex?C=U\Lambda&space;U^{T}" title="C=U\Lambda U^{T}" style="display:inline;vertical-align:text-top;">  （<img src="https://latex.codecogs.com/gif.latex?U" title="U" style="display:inline;vertical-align:text-top;">为特征向量组成的正交矩阵，<img src="https://latex.codecogs.com/gif.latex?\Lambda" title="\Lambda" style="display:inline;vertical-align:text-top;">为对应的特征值组成的对角矩阵）。接下来我们从<a href="https://github.com/DorianZi/algorithm_explained/blob/master/matrix_similarity.md#%E7%9B%B8%E4%BC%BC%E7%9F%A9%E9%98%B5" target="_blank" rel="noopener">相似矩阵</a>的意义去理解，即：从特征向量组成的新基的视角看矩阵C，是<img src="https://latex.codecogs.com/gif.latex?\Lambda" title="\Lambda" style="display:inline;vertical-align:text-top;">的样子。所以<img src="https://latex.codecogs.com/gif.latex?\Lambda" title="\Lambda" style="display:inline;vertical-align:text-top;">是特征向量视角下的新的协方差矩阵。 它的对角线元素表示在特征向量方向的数据散度（方差），非对角线元素表示两个特征向量方向上数据相关性，在这里它为0，说明不相关。</p>
<p>这个结论也正好反过来说明了，我们用找到的特征向量作为新的主元，足以表示数据的特征。接下来要做的当然就是选取最大的m个维度进行降维了。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/19/image_compression_with_SVD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Dorian Zi">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dorian Circle">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/19/image_compression_with_SVD/" class="post-title-link" itemprop="url">Image Compression by SVD</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-03-19 09:53:44 / Modified: 12:05:29" itemprop="dateCreated datePublished" datetime="2019-03-19T09:53:44+08:00">2019-03-19</time>
            

            
              

              
            
          </span>

          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="基于SVD的图像压缩实践"><a href="#基于SVD的图像压缩实践" class="headerlink" title="基于SVD的图像压缩实践"></a>基于SVD的图像压缩实践</h1><p>在<a href="https://github.com/DorianZi/algorithm_explained/blob/master/matrix_SVD_decomposition.md" target="_blank" rel="noopener">之前的介绍</a>里，我们讲过了SVD分解的基本原理。一个mxn的矩阵A可以分解为三个矩阵相乘：</p>
<p>A可以被U和V表示，而其中的奇异值<img src="https://latex.codecogs.com/gif.latex?\lambda_{1},\lambda_{2},...,\lambda_{n}" title="\lambda_{1},\lambda_{2},...,\lambda_{n}" style="display:inline;vertical-align:text-top;">分别为U的列向量和V的行向量的权重：</p>
<p><img src="https://latex.codecogs.com/gif.latex?\underset{m\times&space;n}{A}=\lambda_{1}u_{1}v^{T}_{1}&plus;\lambda_{2}u_{2}v^{T}_{2}&plus;...&plus;\lambda_{n}u_{n}v^{T}_{n}" title="=\lambda_{1}u_{1}v^{T}_{1}+\lambda_{2}u_{2}v^{T}_{2}+...+\lambda_{n}u_{n}v^{T}_{n}" style="display:inline;vertical-align:text-top;"></p>
<p>通常，前k大的奇异值就足以占了所以奇异值90%的比重，这种情况下，我们只需要选择前k个奇异值，对应地选择U的前k个列向量，还有V的前k个行向量，就可以近似表示出矩阵A:</p>
<p><img src="https://latex.codecogs.com/gif.latex?\underset{m\times&space;n}{\tilde{A}}=\underset{m\times&space;k}{(u_{1},u_{2},...,u_{k})}&space;\underset{k\times&space;k}{\begin{bmatrix}&space;\lambda_{1}&&space;&&space;&&space;0&space;\\&space;&&space;\lambda_{2}&&space;&&space;\\&space;&&space;&&space;...&&space;\\&space;&&space;&&space;&&space;\lambda_{k}\end{bmatrix}}&space;\underset{k\times&space;n}{\begin{pmatrix}&space;v^{T}_{1}\\&space;v^{T}_{2}\\&space;...\\&space;v^{T}_{k}&space;\end{pmatrix}&space;}" style="display:inline;vertical-align:text-top;"></p>
<p>在图像处理中，我们把一张图片的每个像素点作为矩阵中的元素，则我们得到一个mxn的矩阵，利用SVD分解，取前k个奇异值，就能实现图像的压缩：</p>
<p><img src="https://github.com/DorianZi/algorithm_explained/raw/master/res/svd_cut.png" style="display:inline;vertical-align:text-top;"></p>
<p>同时我也可以称作其为图像降噪，因为可以认为奇异值非常小的那些项是噪声。<br>Anyway, 直接上代码</p>
<h2 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from optparse import OptionParser</span><br><span class="line">import numpy as np</span><br><span class="line">import sys</span><br><span class="line"></span><br><span class="line"># Get image path</span><br><span class="line">def getOptions():</span><br><span class="line">	parser = OptionParser()</span><br><span class="line">	parser.add_option(&quot;-p&quot;, &quot;--picture&quot;, dest=&quot;picture&quot;, action=&quot;store&quot;, help=&quot;specify the input picture path&quot;)</span><br><span class="line">	parser.add_option(&quot;-c&quot;, &quot;--count&quot;, dest=&quot;count&quot;, action=&quot;store&quot;, help=&quot;specify the required singular value count&quot;)</span><br><span class="line">	options, _ = parser.parse_args()</span><br><span class="line">	if not options.picture:</span><br><span class="line">		sys.exit(&quot;-p/--picture is required!&quot;)</span><br><span class="line">	return options</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compress(K,U,Sigma,VT,verbose=False):</span><br><span class="line">	if verbose:</span><br><span class="line">		print(&quot;Selected singular value count: &#123;&#125;&quot;.format(K))</span><br><span class="line">		print(&quot;Before compression: &#123;&#125; &#123;&#125; &#123;&#125;&quot;.format(U.shape, Sigma.shape, VT.shape))</span><br><span class="line">	U_K = U[:, :K]</span><br><span class="line">	Sigma_K = np.eye(K)*Sigma[:K]   # Because Sigma matrix is a vector instead of a matrix, need to convert to matrix</span><br><span class="line">	VT_K = VT[:K, :]</span><br><span class="line">	if verbose:</span><br><span class="line">		print(&quot;After  compression: &#123;&#125; &#123;&#125; &#123;&#125;&quot;.format(U_K.shape, Sigma_K.shape, VT_K.shape))</span><br><span class="line">	return np.matmul(np.matmul(U_K,Sigma_K), VT_K)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">	options = getOptions()</span><br><span class="line"></span><br><span class="line">	# Read image</span><br><span class="line">	pMatrix= np.array(plt.imread(options.picture))</span><br><span class="line">	print(&quot;Original image shape: &#123;&#125;&quot;.format(pMatrix.shape))</span><br><span class="line"></span><br><span class="line">	# Do SVD decomposition for each channel of RGB</span><br><span class="line">	R, G, B = pMatrix[:,:,0], pMatrix[:,:,1], pMatrix[:,:,2]</span><br><span class="line">	U_R,Sigma_R,VT_R = np.linalg.svd(R)</span><br><span class="line">	U_G,Sigma_G,VT_G = np.linalg.svd(G)</span><br><span class="line">	U_B,Sigma_B,VT_B = np.linalg.svd(B)</span><br><span class="line"></span><br><span class="line">	# compress by top K singular values</span><br><span class="line">	K = int(options.count) if options.count else 100</span><br><span class="line">	R_new = compress(K,U_R,Sigma_R,VT_R,verbose=True)</span><br><span class="line">	G_new = compress(K,U_G,Sigma_G,VT_G)</span><br><span class="line">	B_new = compress(K,U_B,Sigma_B,VT_B)</span><br><span class="line">	pMatrix_new = np.stack((R_new,G_new,B_new),2)  # Compose R,G,B channels back to complete image</span><br><span class="line">	print(&quot;Compresses image shape: &#123;&#125;&quot;.format(pMatrix_new.shape))</span><br><span class="line"></span><br><span class="line">	# show image after compress</span><br><span class="line">	plt.imshow(pMatrix_new)</span><br><span class="line">	plt.show()</span><br><span class="line">	sys.exit()</span><br></pre></td></tr></table></figure>
<h2 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ python imageCompress.py -p pic_1.png -c 1000</span><br><span class="line">     Original image shape: (1440L, 1080L, 4L)</span><br><span class="line">     Selected singular value count: 1000</span><br><span class="line">     Before compression: (1440L, 1440L) (1080L,) (1080L, 1080L)</span><br><span class="line">     After  compression: (1440L, 1000L) (1000L, 1000L) (1000L, 1080L)</span><br><span class="line">     Compresses image shape: (1440L, 1080L, 3L)</span><br></pre></td></tr></table></figure>
<p>当K=10，即选取10个奇异值进行压缩：</p>
<p><img src="https://github.com/DorianZi/algorithm_explained/blob/master/res/SVD_k_10.png?raw=true" style="display:inline;vertical-align:text-top;"></p>
<p>当K=30</p>
<p><img src="https://github.com/DorianZi/algorithm_explained/blob/master/res/SVD_k_30.png?raw=true" style="display:inline;vertical-align:text-top;"></p>
<p>当K=50</p>
<p><img src="https://github.com/DorianZi/algorithm_explained/blob/master/res/SVD_k_50.png?raw=true" style="display:inline;vertical-align:text-top;"></p>
<p>当K=100</p>
<p><img src="https://github.com/DorianZi/algorithm_explained/blob/master/res/SVD_k_100.png?raw=true" style="display:inline;vertical-align:text-top;"></p>
<p>当K=500</p>
<p><img src="https://github.com/DorianZi/algorithm_explained/blob/master/res/SVD_k_500.png?raw=true" style="display:inline;vertical-align:text-top;"></p>
<p>可见，当使用前100个奇异值（约占全部奇异值数量的10%），就可以比较好地表示原图像了</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/19/matrix_SVD_decomposition/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Dorian Zi">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dorian Circle">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/19/matrix_SVD_decomposition/" class="post-title-link" itemprop="url">SVD Decomposition</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-03-19 08:55:44 / Modified: 11:51:59" itemprop="dateCreated datePublished" datetime="2019-03-19T08:55:44+08:00">2019-03-19</time>
            

            
              

              
            
          </span>

          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>SVD分解即奇异值分解， 可以从特征值分解推导而来。先理解特征值分解</p>
<h1 id="特征值和特征向量"><a href="#特征值和特征向量" class="headerlink" title="特征值和特征向量"></a>特征值和特征向量</h1><p>对矩阵A，存在特征向量<img src="https://latex.codecogs.com/gif.latex?\alpha" title="\alpha" style="display:inline;vertical-align:text-top;">和特征值<img src="https://latex.codecogs.com/gif.latex?\lambda" title="\lambda" style="display:inline;vertical-align:text-top;">满足：<img src="https://latex.codecogs.com/gif.latex?A\alpha=\lambda\alpha" title="A\alpha=\lambda\alpha" style="display:inline;vertical-align:text-top;"></p>
<p>如果把矩阵A理解为线性变换，那么上式表示：可以找到向量<img src="https://latex.codecogs.com/gif.latex?\alpha" title="\alpha" style="display:inline;vertical-align:text-top;">使得A只能对它进行<img src="https://latex.codecogs.com/gif.latex?\lambda" title="\lambda" style="display:inline;vertical-align:text-top;">倍的拉伸。</p>
<p>以<font size="5">基变换</font>来理解。我们先构建一个“绝对”坐标系（如下图）。为了便于计算，设置原基为<img src="https://latex.codecogs.com/gif.latex?\binom{1}{0}" title="\binom{1}{0}" style="display:inline;vertical-align:text-top;">和<img src="https://latex.codecogs.com/gif.latex?\binom{0}{1}" title="\binom{0}{1}" style="display:inline;vertical-align:text-top;">，正好与“绝对”坐标系重叠。</p>
<p>通过矩阵<img src="https://latex.codecogs.com/gif.latex?\begin{bmatrix}&space;3&&space;1\\&space;0&2&space;\end{bmatrix}" title="\begin{bmatrix} 3& 1\\ 0&2 \end{bmatrix}" style="display:inline;vertical-align:text-top;">换基，可直接计算得到实际上是将原基变换为新基<img src="https://latex.codecogs.com/gif.latex?\binom{3}{0}" title="\binom{3}{0}" style="display:inline;vertical-align:text-top;">和<img src="https://latex.codecogs.com/gif.latex?\binom{1}{2}" title="\binom{1}{2}" style="display:inline;vertical-align:text-top;">：</p>
<p><img src="https://latex.codecogs.com/gif.latex?\begin{bmatrix}&space;3&space;&&space;1\\&space;0&space;&&space;2&space;\end{bmatrix}\begin{bmatrix}&space;1\\&space;0&space;\end{bmatrix}=\begin{bmatrix}&space;3\\&space;0&space;\end{bmatrix}" title="\begin{bmatrix} 3 & 1\\ 0 & 2 \end{bmatrix}\begin{bmatrix} 1\\ 0 \end{bmatrix}=\begin{bmatrix} 3\\ 0 \end{bmatrix}" style="display:inline;vertical-align:text-top;"></p>
<p><img src="https://latex.codecogs.com/gif.latex?\begin{bmatrix}&space;3&space;&&space;1\\&space;0&space;&&space;2&space;\end{bmatrix}\begin{bmatrix}&space;0\\&space;1&space;\end{bmatrix}=\begin{bmatrix}&space;1\\&space;2&space;\end{bmatrix}" title="\begin{bmatrix} 3 & 1\\ 0 & 2 \end{bmatrix}\begin{bmatrix} 0\\ 1 \end{bmatrix}=\begin{bmatrix} 1\\ 2 \end{bmatrix}" style="display:inline;vertical-align:text-top;"></p>
<p>换基不影响坐标值，所以原基下的坐标(1,1)，变换后在新基下的坐标也是(1,1），而在“绝对”坐标系中坐标为(4,2)</p>
<p><img src="https://latex.codecogs.com/gif.latex?\begin{bmatrix}&space;3&space;&&space;1\\&space;0&space;&&space;2&space;\end{bmatrix}\begin{bmatrix}&space;1\\&space;1&space;\end{bmatrix}=\begin{bmatrix}&space;4\\&space;2&space;\end{bmatrix}" title="\begin{bmatrix} 3 & 1\\ 0 & 2 \end{bmatrix}\begin{bmatrix} 1\\ 1 \end{bmatrix}=\begin{bmatrix} 4\\ 2 \end{bmatrix}" style="display:inline;vertical-align:text-top;"></p>
<p><img src="https://github.com/DorianZi/algorithm_explained/raw/master/res/pic_3.png" style="display:inline;vertical-align:text-top;"></p>
<p>在基变换体系下，我们无法直接获得特征向量和特征值的几何意义，以空间拉伸来解释会获得更直观理解。</p>
<p>以<font size="5">空间拉伸</font>来理解，以下线性变换将正方形围住的空间变换成普通平行四边形。大部分向量都被施加了旋转和拉伸（<font color="#8B0000">褐色</font>），只有特征向量<img src="https://latex.codecogs.com/gif.latex?\alpha_{1},\alpha_{2}" title="\alpha_{1},\alpha_{2}" style="display:inline;vertical-align:text-top;">（<font color="#008000">绿色</font>和<font color="#0000FF">蓝色</font>）所在方向的向量只进行了拉伸，且拉伸效果为相应的特征值<img src="https://latex.codecogs.com/gif.latex?\lambda_{1},\lambda_{2}" title="\lambda_{1},\lambda_{2}" style="display:inline;vertical-align:text-top;"></p>
<p><img src="https://github.com/DorianZi/algorithm_explained/raw/master/res/pic1.png" style="display:inline;vertical-align:text-top;"></p>
<p>再回到最上面的图,如果继续不断地对空间施加矩阵A的线性变换(<img src="https://latex.codecogs.com/gif.latex?A^{n}" title="A^{n}" style="display:inline;vertical-align:text-top;">), 那么特征向量所在方向的向量（<font color="#008000">绿色</font>和<font color="#0000FF">蓝色</font>）会幂级拉伸，非特征向量所以在方向的向量（<font color="#8B0000">褐色</font>）会越来越<font size="5">接近</font>特征向量方向。特殊情况，当<img src="https://latex.codecogs.com/gif.latex?\lambda=1" title="\lambda=1" style="display:inline;vertical-align:text-top;">时，系统会<font size="5">稳定</font>在<font size="5">最大</font>特征值对应的特征向量方向。<br><img src="https://github.com/DorianZi/algorithm_explained/raw/master/res/pic4.png" style="display:inline;vertical-align:text-top;"></p>
<h1 id="矩阵对角化"><a href="#矩阵对角化" class="headerlink" title="矩阵对角化"></a>矩阵对角化</h1><p>将矩阵对角化为特征向量为列的矩阵<img src="https://latex.codecogs.com/gif.latex?P" title="P" style="display:inline;vertical-align:text-top;">和对角矩阵<img src="https://latex.codecogs.com/gif.latex?\Lambda" title="\Lambda" style="display:inline;vertical-align:text-top;">的乘积：</p>
<p><img src="https://latex.codecogs.com/gif.latex?A=P\Lambda&space;P^{-1}" title="A=P\Lambda P^{-1}" style="display:inline;vertical-align:text-top;"></p>
<p>其中<img src="https://latex.codecogs.com/gif.latex?P" title="P" style="display:inline;vertical-align:text-top;">的列向量为特征向量，<img src="https://latex.codecogs.com/gif.latex?\Lambda" title="\Lambda" style="display:inline;vertical-align:text-top;">为特征值矩阵。</p>
<h2 id="数学推导"><a href="#数学推导" class="headerlink" title="数学推导"></a>数学推导</h2><p>对角化推导如下：</p>
<p>根据特征值/特征向量定义：</p>
<p><img src="https://latex.codecogs.com/gif.latex?A\alpha_{1}=\lambda_{1}\alpha_{1},A\alpha_{2}=\lambda_{2}\alpha_{2},...,A\alpha_{n}=\lambda_{n}\alpha_{n}" title="A\alpha_{1}=\lambda_{1}\alpha_{1},A\alpha_{2}=\lambda_{2}\alpha_{2},...,A\alpha_{n}=\lambda_{n}\alpha_{n}" style="display:inline;vertical-align:text-top;"></p>
<p>合并为矩阵形式：</p>
<p><img src="https://latex.codecogs.com/gif.latex?A(\alpha_{1},\alpha_{2},...,\alpha_{n})&space;=&space;(\lambda_{1}\alpha_{1},\lambda_{2}\alpha_{2},...,\lambda_{n}\alpha_{n})" title="A(\alpha_{1},\alpha_{2},...,\alpha_{n}) = (\lambda_{1}\alpha_{1},\lambda_{2}\alpha_{2},...,\lambda_{n}\alpha_{n})" style="display:inline;vertical-align:text-top;"></p>
<p><img src="https://latex.codecogs.com/gif.latex?A(\alpha_{1},\alpha_{2},...,\alpha_{n})&space;=&space;(\alpha_{1},\alpha_{2},...,\alpha_{n})&space;\begin{bmatrix}&space;\lambda_{1}&&space;&&space;&&space;\\&space;&&space;\lambda_{2}&&space;&&space;\\&space;&&space;&&space;...&&space;\\&space;&&space;&&space;&\lambda_{n}&space;\end{bmatrix}" title="A(\alpha_{1},\alpha_{2},...,\alpha_{n}) = (\alpha_{1},\alpha_{2},...,\alpha_{n}) \begin{bmatrix} \lambda_{1}& & & \\ & \lambda_{2}& & \\ & & ...& \\ & & &\lambda_{n} \end{bmatrix}" style="display:inline;vertical-align:text-top;"></p>
<p><img src="https://latex.codecogs.com/gif.latex?A=(\alpha_{1},\alpha_{2},...,\alpha_{n})&space;\begin{bmatrix}&space;\lambda_{1}&&space;&&space;&&space;\\&space;&&space;\lambda_{2}&&space;&&space;\\&space;&&space;&&space;...&&space;\\&space;&&space;&&space;&\lambda_{n}&space;\end{bmatrix}&space;(\alpha_{1},\alpha_{2},...,\alpha_{n})^{-1}" title="A=(\alpha_{1},\alpha_{2},...,\alpha_{n}) \begin{bmatrix} \lambda_{1}& & & \\ & \lambda_{2}& & \\ & & ...& \\ & & &\lambda_{n} \end{bmatrix} (\alpha_{1},\alpha_{2},...,\alpha_{n})^{-1}" style="display:inline;vertical-align:text-top;"></p>
<h2 id="几何推导"><a href="#几何推导" class="headerlink" title="几何推导"></a>几何推导</h2><p>前提知识：从新的一组基<img src="https://latex.codecogs.com/gif.latex?(\alpha_{1},\alpha_{2},..,\alpha_{n})" title="(\alpha_{1},\alpha_{2},..,\alpha_{n})" style="display:inline;vertical-align:text-top;">的角度来看待一个线性变换A为：<img src="https://latex.codecogs.com/gif.latex?(\alpha_{1},\alpha_{2},...,\alpha_{n})^{-1}A(\alpha_{1},\alpha_{2},...,\alpha_{n})" title="(\alpha_{1},\alpha_{2},...,\alpha_{n})^{-1}A(\alpha_{1},\alpha_{2},...,\alpha_{n})" style="display:inline;vertical-align:text-top;"></p>
<p>那么基于这个前提知识，我们抽取出特征向量组成一组新基<img src="https://latex.codecogs.com/gif.latex?(\alpha_{1},\alpha_{2},..,\alpha_{n})" title="(\alpha_{1},\alpha_{2},..,\alpha_{n})" style="display:inline;vertical-align:text-top;">, 从这组新基的角度来看待线性变换A为等式左边；等式右边一定为对角矩阵，且对角元素值为特征值。这是因为从特征向量的角度看来，A只是完成了拉伸，而没有旋转：</p>
<p><img src="https://latex.codecogs.com/gif.latex?(\alpha_{1},\alpha_{2},...,\alpha_{n})^{-1}A(\alpha_{1},\alpha_{2},...,\alpha_{n})=\begin{bmatrix}&space;\lambda_{1}&space;&&space;&&space;&&space;\\&space;&&space;\lambda_{2}&space;&&space;&&space;\\&space;&&space;&&space;...&&space;\\&space;&&space;&&space;&&space;\lambda_{n}&space;\end{bmatrix}" title="(\alpha_{1},\alpha_{2},...,\alpha_{n})^{-1}A(\alpha_{1},\alpha_{2},...,\alpha_{n})=\begin{bmatrix} \lambda_{1} & & & \\ & \lambda_{2} & & \\ & & ...& \\ & & & \lambda_{n} \end{bmatrix}" style="display:inline;vertical-align:text-top;"></p>
<p>另外，从过程上理解，对任意一个向量<img src="https://latex.codecogs.com/gif.latex?\beta" title="\beta" style="display:inline;vertical-align:text-top;">施加矩阵A变换，相当于连续施加三个变换:<img src="https://latex.codecogs.com/gif.latex?P^{-1}" title="P^{-1}" style="display:inline;vertical-align:text-top;"> -&gt; <img src="https://latex.codecogs.com/gif.latex?\Lambda" title="\Lambda" style="display:inline;vertical-align:text-top;"> -&gt; <img src="https://latex.codecogs.com/gif.latex?P" title="P" style="display:inline;vertical-align:text-top;"> :</p>
<p><img src="https://latex.codecogs.com/gif.latex?A\beta=P\Lambda&space;P^{-1}\beta" title="A\beta=P\Lambda P^{-1}\beta" style="display:inline;vertical-align:text-top;"></p>
<p>特殊情况下，如果<img src="https://latex.codecogs.com/gif.latex?P" title="P" style="display:inline;vertical-align:text-top;">和<img src="https://latex.codecogs.com/gif.latex?P^{-1}" title="P^{-1}" style="display:inline;vertical-align:text-top;">为正交矩阵，则只有旋转效果,且它们互为反向旋转；<img src="https://latex.codecogs.com/gif.latex?\Lambda" title="\Lambda" style="display:inline;vertical-align:text-top;">为对角矩阵，只有拉伸效果。所以改变换为:</p>
<p>旋转 -&gt; 拉伸 -&gt; 转回来 </p>
<h2 id="SVD奇异值分解"><a href="#SVD奇异值分解" class="headerlink" title="SVD奇异值分解"></a>SVD奇异值分解</h2><p>SVD = Singular Value Decomposition 即奇异值分解。特征值可以理解为奇异值的一种特殊情况，即矩阵为方阵的情况。在矩阵非方阵的时候，我们将类似的分解称作奇异值分解</p>
<p>我们有一个非方阵的维度为<img src="https://latex.codecogs.com/gif.latex?m\times&space;n(m<n)" title="m\times n(m<n)" style="display:inline;vertical-align:text-top;">的矩阵<img src="https://latex.codecogs.com/gif.latex?\underset{m\times&space;n}{A}" title="\underset{m\times n}{A}" style="display:inline;vertical-align:text-top;">，希望能够对角化。采用上面的特征矩阵分解方式是不可能了，但是我们可以利用<img src="https://latex.codecogs.com/gif.latex?A^{T}A" title="A^{T}A" style="display:inline;vertical-align:text-top;">（<img src="https://latex.codecogs.com/gif.latex?AA^{T}" title="AA^{T}" style="display:inline;vertical-align:text-top;">）为实对称方阵的特性来获得不同的特征矩阵分解：</p>
<p><img src="https://latex.codecogs.com/gif.latex?\underset{n\times&space;m}{A^{T}}\underset{m\times&space;n}{A}=\underset{n\times&space;n}{U}\underset{n\times&space;n}{\Lambda&space;_{1}}\underset{n\times&space;n}{U^{-1}}=\underset{n\times&space;n}{U}\underset{n\times&space;n}{\Lambda&space;_{1}}\underset{n\times&space;n}{U^{T}}" style="display:inline;vertical-align:text-top;"></p>
<p><img src="https://latex.codecogs.com/gif.latex?\underset{m\times&space;n}{A}\underset{n\times&space;m}{A^{T}}=\underset{m\times&space;m}{V}\underset{m\times&space;m}{\Lambda&space;_{2}}\underset{m\times&space;m}{V^{-1}}=\underset{m\times&space;m}{V}\underset{m\times&space;m}{\Lambda&space;_{2}}\underset{m\times&space;m}{V^{T}}" style="display:inline;vertical-align:text-top;"></p>
<p>可以推得(推导过程TBD)：</p>
<p><img src="https://latex.codecogs.com/gif.latex?\underset{m\times&space;n}{A}=\underset{m\times&space;m}{U}\underset{m\times&space;n}{\sum}\underset{n\times&space;n}{V^{T}}" style="display:inline;vertical-align:text-top;"></p>
<p><img src="https://github.com/DorianZi/algorithm_explained/blob/master/res/SVD_graph.png?raw=true" style="display:inline;vertical-align:text-top;"></p>
<h3 id="奇异值求解推导"><a href="#奇异值求解推导" class="headerlink" title="奇异值求解推导"></a>奇异值求解推导</h3><p>接下来我们来求新的对角矩阵<img src="https://latex.codecogs.com/gif.latex?\underset{m\times&space;n}{\sum}" title="\underset{m\times n}{\sum}" style="display:inline;vertical-align:text-top;">的对角元素，即奇异值<img src="https://latex.codecogs.com/gif.latex?\lambda_{1},\lambda_{2},...,\lambda_{n}" title="\lambda_{1},\lambda_{2},...,\lambda_{n}" style="display:inline;vertical-align:text-top;"></p>
<p>首先了解一下<img src="https://latex.codecogs.com/gif.latex?\underset{m\times&space;n}{\sum}" title="\underset{m\times n}{\sum}" style="display:inline;vertical-align:text-top;">的形状：</p>
<p><img src="https://latex.codecogs.com/gif.latex?\underset{m\times&space;n}{\sum&space;}=&space;\underset{m\times&space;n}{\begin{bmatrix}&space;\lambda_{1}&&space;&&space;&&space;&&space;0\\&space;&&space;\lambda_{2}&&space;&&space;&&space;\\&space;&&space;&&space;...&&space;&&space;\\&space;&&space;&&space;&&space;&&space;\lambda_{n}\\&space;&&space;&&space;&&space;&&space;0\\&space;&&space;&&space;&&space;&&space;...\\&space;0&space;&&space;&&space;&&space;&&space;0\\&space;\end{bmatrix}&space;}" title="\underset{m\times n}{\sum }= \underset{m\times n}{\begin{bmatrix} \lambda_{1}& & & & 0\\ & \lambda_{2}& & & \\ & & ...& & \\ & & & & \lambda_{n}\\ & & & & 0\\ & & & & ...\\ 0 & & & & 0\\ \end{bmatrix} }" style="display:inline;vertical-align:text-top;"></p>
<p>开始推导：</p>
<p><img src="https://latex.codecogs.com/gif.latex?A=U\sum&space;V^{T}\&space;\&space;\&space;\&space;\&space;\&space;=>\&space;\&space;\&space;\&space;\&space;\&space;AV=U\sum&space;V^{T}V&space;\&space;\&space;\&space;\&space;\&space;\underset{=======>}{V^{T}V=I}&space;\&space;\&space;\&space;\&space;AV=U\sum" title="A=U\sum V^{T}\ \ \ \ \ \ =>\ \ \ \ \ \ AV=U\sum V^{T}V \ \ \ \ \ \underset{=======>}{V^{T}V=I} \ \ \ \ AV=U\sum" style="display:inline;vertical-align:text-top;"></p>
<p><img src="https://latex.codecogs.com/gif.latex?AV=U\sum" title="AV=U\sum" style="display:inline;vertical-align:text-top;"> 写成列向量的组合形式：</p>
<p><img src="https://latex.codecogs.com/gif.latex?A(v_{1},v_{2},...,v_{n})=(u_{1},u_{2},...,u_{n},...,u_{m})&space;\underset{m\times&space;n}{\begin{bmatrix}&space;\lambda_{1}&&space;&&space;&&space;&&space;0\\&space;&&space;\lambda_{2}&&space;&&space;&&space;\\&space;&&space;&&space;...&&space;&&space;\\&space;&&space;&&space;&&space;&&space;\lambda_{n}\\&space;&&space;&&space;&&space;&&space;0\\&space;&&space;&&space;&&space;&&space;...\\&space;0&space;&&space;&&space;&&space;&&space;0\\&space;\end{bmatrix}&space;}" style="display:inline;vertical-align:text-top;"></p>
<p>=&gt;</p>
<p><img src="https://latex.codecogs.com/gif.latex?(Av_{1},Av_{2},...,Av_{n})=(\lambda_{1}u_{1},\lambda_{1}u_{1},...,\lambda_{n}u_{n})" title="(Av_{1},Av_{2},...,Av_{n})=(\lambda_{1}u_{1},\lambda_{1}u_{1},...,\lambda_{n}u_{n})" style="display:inline;vertical-align:text-top;"></p>
<p>=&gt;</p>
<p><img src="https://latex.codecogs.com/gif.latex?\lambda_{1}=\frac{Av_{1}}{u_{1}},&space;\&space;\&space;\&space;\lambda_{2}=\frac{Av_{2}}{u_{2}}&space;\&space;\&space;\&space;...&space;\&space;\&space;\&space;\&space;\&space;\lambda_{n}=\frac{Av_{n}}{u_{n}}" title="\lambda_{1}=\frac{Av_{1}}{u_{1}}, \ \ \ \lambda_{2}=\frac{Av_{2}}{u_{2}} \ \ \ ... \ \ \ \ \ \lambda_{n}=\frac{Av_{n}}{u_{n}}" style="display:inline;vertical-align:text-top;"></p>
<p>另外一种推导方式——</p>
<p>在最开始的奇异值分解推导中，我们已经通过<img src="https://latex.codecogs.com/gif.latex?\Lambda&space;_{1}" title="\Lambda _{1}" style="display:inline;vertical-align:text-top;">和<img src="https://latex.codecogs.com/gif.latex?\Lambda&space;_{2}" title="\Lambda _{2}" style="display:inline;vertical-align:text-top;">来表示<img src="https://latex.codecogs.com/gif.latex?AA^{T}" title="AA^{T}" style="display:inline;vertical-align:text-top;">和<img src="https://latex.codecogs.com/gif.latex?A^{T}A" title="A^{T}A" style="display:inline;vertical-align:text-top;">，现在我们通过<img src="https://latex.codecogs.com/gif.latex?\sum" title="\sum" style="display:inline;vertical-align:text-top;">来表示<img src="https://latex.codecogs.com/gif.latex?AA^{T}" title="AA^{T}" style="display:inline;vertical-align:text-top;">和<img src="https://latex.codecogs.com/gif.latex?A^{T}A" title="A^{T}A" style="display:inline;vertical-align:text-top;">。这样我们就能知道<img src="https://latex.codecogs.com/gif.latex?\Lambda&space;_{1},\Lambda&space;_{2}" title="\Lambda _{1},\Lambda _{2}" style="display:inline;vertical-align:text-top;">和<img src="https://latex.codecogs.com/gif.latex?\sum" title="\sum" style="display:inline;vertical-align:text-top;">的关系</p>
<p><img src="https://latex.codecogs.com/gif.latex?A=U\sum&space;V^{T}&space;\&space;\&space;\&space;\&space;=&space;>&space;\&space;\&space;\&space;A^{T}&space;=V(\sum)^{T}U^{T}" title="A=U\sum V^{T} \ \ \ \ = > \ \ \ A^{T} =V(\sum)^{T}U^{T}" style="display:inline;vertical-align:text-top;"></p>
<p><img src="https://latex.codecogs.com/gif.latex?AA^{T}=U\sum&space;V^{T}V(\sum)^{T}U^{T}&space;=U&space;\underset{m\times&space;m}{\begin{bmatrix}&space;\lambda_{1}^{2}&&space;&&space;&&space;&&space;&&space;&0&space;\\&space;&&space;\lambda_{1}^{2}&&space;&&space;&&space;&&space;&&space;\\&space;&&space;&&space;...&&space;&&space;&&space;&&space;\\&space;&&space;&&space;&&space;\lambda_{n}^{2}&&space;&&space;&\\&space;&&space;&&space;&&space;&&space;0&&space;&\\&space;&&space;&&space;&&space;&&space;&&space;...&\\&space;0&space;&&space;&&space;&&space;&&space;&&space;&0\\&space;\end{bmatrix}}U^{T}" style="display:inline;vertical-align:text-top;"></p>
<p><img src="https://latex.codecogs.com/gif.latex?A^{T}A=V(\sum)^{T}U^{T}U\sum&space;V^{T}&space;=V&space;\underset{n\times&space;n}{\begin{bmatrix}&space;\lambda_{1}^{2}&&space;&&space;&&space;\\&space;&&space;\lambda_{1}^{2}&&space;&&space;\\&space;&&space;&&space;...&&space;\\&space;&&space;&&space;&&space;\lambda_{n}^{2}&space;\end{bmatrix}}V^{T}" style="display:inline;vertical-align:text-top;"></p>
<p>BTW，上面用到了<img src="https://latex.codecogs.com/gif.latex?U^{T}U=I,\&space;V^{T}V=I" title="U^{T}U=I,\ \ \ V^{T}V=I" style="display:inline;vertical-align:text-top;">的条件。</p>
<p>由此我们得到奇异值为特征值的开根。</p>
<h3 id="SVD分解的意义"><a href="#SVD分解的意义" class="headerlink" title="SVD分解的意义"></a>SVD分解的意义</h3><p>A可以被U和V表示，而其中的奇异值<img src="https://latex.codecogs.com/gif.latex?\lambda_{1},\lambda_{2},...,\lambda_{n}" title="\lambda_{1},\lambda_{2},...,\lambda_{n}" style="display:inline;vertical-align:text-top;">分别为U的列向量和V的行向量的权重：</p>
<p><img src="https://latex.codecogs.com/gif.latex?\underset{m\times&space;n}{A}=\underset{m\times&space;m}{(u_{1},u_{2},...,u_{n},...,u_{m})}&space;\underset{m\times&space;n}{\begin{bmatrix}&space;\lambda_{1}&&space;&&space;&&space;0&space;\\&space;&&space;\lambda_{2}&&space;&&space;\\&space;&&space;&&space;...&&space;\\&space;&&space;&&space;&&space;\lambda_{n}\\&space;&&space;&&space;&&space;0\\&space;&&space;&&space;&&space;...\\&space;0&&space;&&space;&&space;0\\&space;\end{bmatrix}}&space;\underset{n\times&space;n}{\begin{pmatrix}&space;v^{T}_{1}\\&space;v^{T}_{2}\\&space;...\\&space;v^{T}_{n}&space;\end{pmatrix}&space;}" title="\underset{m\times n}{A}=\underset{m\times m}{(u_{1},u_{2},...,u_{n},...,u_{m})} \underset{m\times n}{\begin{bmatrix} \lambda_{1}& & & 0 \\ & \lambda_{2}& & \\ & & ...& \\ & & & \lambda_{n}\\ & & & 0\\ & & & ...\\ 0& & & 0\\ \end{bmatrix}} \underset{n\times n}{\begin{pmatrix} v^{T}_{1}\\ v^{T}_{2}\\ ...\\ v^{T}_{n} \end{pmatrix} }" style="display:inline;vertical-align:text-top;"></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src="https://latex.codecogs.com/gif.latex?=(u_{1}\lambda_{1},u_{2}\lambda_{2},...,u_{n}\lambda_{n})&space;\underset{n\times&space;n}{\begin{pmatrix}&space;v^{T}_{1}\\&space;v^{T}_{2}\\&space;...\\&space;v^{T}_{n}&space;\end{pmatrix}&space;}" title="=(u_{1}\lambda_{1},u_{2}\lambda_{2},...,u_{n}\lambda_{n}) \underset{n\times n}{\begin{pmatrix} v^{T}_{1}\\ v^{T}_{2}\\ ...\\ v^{T}_{n} \end{pmatrix} }" style="display:inline;vertical-align:text-top;"></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src="https://latex.codecogs.com/gif.latex?=\lambda_{1}u_{1}v^{T}_{1}&plus;\lambda_{2}u_{2}v^{T}_{2}&plus;...&plus;\lambda_{n}u_{n}v^{T}_{n}" title="=\lambda_{1}u_{1}v^{T}_{1}+\lambda_{2}u_{2}v^{T}_{2}+...+\lambda_{n}u_{n}v^{T}_{n}" style="display:inline;vertical-align:text-top;"></p>
<p>通常，前k大的奇异值就足以占了所以奇异值90%的比重，这种情况下，我们只需要选择前k个奇异值，对应地选择U的前k个列向量，还有V的前k个行向量，就可以近似表示出矩阵A:</p>
<p><img src="https://latex.codecogs.com/gif.latex?\underset{m\times&space;n}{\tilde{A}}=\underset{m\times&space;k}{(u_{1},u_{2},...,u_{k})}&space;\underset{k\times&space;k}{\begin{bmatrix}&space;\lambda_{1}&&space;&&space;&&space;0&space;\\&space;&&space;\lambda_{2}&&space;&&space;\\&space;&&space;&&space;...&&space;\\&space;&&space;&&space;&&space;\lambda_{k}\end{bmatrix}}&space;\underset{k\times&space;n}{\begin{pmatrix}&space;v^{T}_{1}\\&space;v^{T}_{2}\\&space;...\\&space;v^{T}_{k}&space;\end{pmatrix}&space;}" style="display:inline;vertical-align:text-top;"></p>
<p><img src="https://github.com/DorianZi/algorithm_explained/raw/master/res/svd_cut.png" style="display:inline;vertical-align:text-top;"></p>
<p>这样一种降维思想在图像压缩，推荐系统里面有着非常广泛的应用。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://www.bilibili.com/video/av6540378/" target="_blank" rel="noopener">https://www.bilibili.com/video/av6540378/</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/19/matrix_similarity/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Dorian Zi">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dorian Circle">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/19/matrix_similarity/" class="post-title-link" itemprop="url">Matrix Similarity</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-03-19 08:53:44 / Modified: 11:48:51" itemprop="dateCreated datePublished" datetime="2019-03-19T08:53:44+08:00">2019-03-19</time>
            

            
              

              
            
          </span>

          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="相似矩阵"><a href="#相似矩阵" class="headerlink" title="相似矩阵"></a>相似矩阵</h1><p>相似矩阵表达的是：同一个线性变换在不同基下的表达。<br>下面的推导中，A矩阵和B矩阵相似，也就是说他们是同一个线性变换在不同基下的表达</p>
<h2 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h2><p>见下图</p>
<p><img src="https://github.com/DorianZi/algorithm_explained/raw/master/res/matrix_similarity.jpg"></p>
<p>首先我们脱离任何基，在“绝对”坐标系里面描述一个线性变换：线性变换将向量<img src="https://latex.codecogs.com/gif.latex?\underset{\alpha}{\rightarrow}" title="\underset{a}{\rightarrow}" style="display:inline;vertical-align:text-top;">变换为<img src="https://latex.codecogs.com/gif.latex?\underset{\beta}{\rightarrow}" title="\underset{\beta}{\rightarrow}" style="display:inline;vertical-align:text-top;"></p>
<p>接下来我们分别在不同的基里描述这个线性变换：</p>
<p>在基<img src="https://latex.codecogs.com/gif.latex?(\underset{i}{\rightarrow}&space;,\underset{j}{\rightarrow})" title="(\underset{i}{\rightarrow} ,\underset{j}{\rightarrow})" style="display:inline;vertical-align:text-top;">下，上述变换描述为：线性变换A将<img src="https://latex.codecogs.com/gif.latex?\underset{v}{\rightarrow}" title="\underset{v}{\rightarrow}" style="display:inline;vertical-align:text-top;">变换为<img src="https://latex.codecogs.com/gif.latex?A\underset{v}{\rightarrow}" title="A\underset{v}{\rightarrow}" style="display:inline;vertical-align:text-top;"></p>
<p>同时由于我们知道向量<img src="https://latex.codecogs.com/gif.latex?\underset{a}{\rightarrow}" title="\underset{a}{\rightarrow}" style="display:inline;vertical-align:text-top;">和<img src="https://latex.codecogs.com/gif.latex?\underset{v}{\rightarrow}" title="\underset{v}{\rightarrow}" style="display:inline;vertical-align:text-top;">的关系是：<img src="https://latex.codecogs.com/gif.latex?(\underset{i}{\rightarrow},\underset{j}{\rightarrow})\underset{a}{\rightarrow}=\underset{v}{\rightarrow}" title="(\underset{i}{\rightarrow},\underset{j}{\rightarrow})\underset{a}{\rightarrow}=\underset{v}{\rightarrow}" style="display:inline;vertical-align:text-top;"></p>
<p>所以该变换可以进一步表述为：线性变换A将<img src="https://latex.codecogs.com/gif.latex?(\underset{i}{\rightarrow},\underset{j}{\rightarrow})\underset{a}{\rightarrow}" title="(\underset{i}{\rightarrow},\underset{j}{\rightarrow})\underset{a}{\rightarrow}" style="display:inline;vertical-align:text-top;">变换为<img src="https://latex.codecogs.com/gif.latex?A(\underset{i}{\rightarrow},\underset{j}{\rightarrow})\underset{\alpha}{\rightarrow}" title="A(\underset{i}{\rightarrow},\underset{j}{\rightarrow})\underset{\alpha}{\rightarrow}" style="display:inline;vertical-align:text-top;"></p>
<p>同理在新基下<img src="https://latex.codecogs.com/gif.latex?(\underset{i'}{\rightarrow}&space;,\underset{j'}{\rightarrow})" title="(\underset{i'}{\rightarrow} ,\underset{j'}{\rightarrow})" style="display:inline;vertical-align:text-top;">该变换表述为：线性变换B将<img src="https://latex.codecogs.com/gif.latex?(\underset{i'}{\rightarrow},\underset{j'}{\rightarrow})\underset{\alpha}{\rightarrow}" title="(\underset{i'}{\rightarrow},\underset{j'}{\rightarrow})\underset{\alpha}{\rightarrow}" style="display:inline;vertical-align:text-top;">变换为<img src="https://latex.codecogs.com/gif.latex?B(\underset{i'}{\rightarrow},\underset{j'}{\rightarrow})\underset{\alpha}{\rightarrow}" title="B(\underset{i'}{\rightarrow},\underset{j'}{\rightarrow})\underset{\alpha}{\rightarrow}" style="display:inline;vertical-align:text-top;"></p>
<p>如果我们知道两组基的转换关系为<img src="https://latex.codecogs.com/gif.latex?P^{-1}" title="P^{-1}" style="display:inline;vertical-align:text-top;">，那么有：</p>
<p><img src="https://latex.codecogs.com/gif.latex?P^{-1}(\underset{i}{\rightarrow},\underset{j}{\rightarrow})=(\underset{i'}{\rightarrow},\underset{j'}{\rightarrow})" title="P^{-1}(\underset{i}{\rightarrow},\underset{j}{\rightarrow})=(\underset{i'}{\rightarrow},\underset{j'}{\rightarrow})" style="display:inline;vertical-align:text-top;"> ①</p>
<p>同时，线性变换之后得到的向量也应该满足基之间的转换关系：</p>
<p><img src="https://latex.codecogs.com/gif.latex?P^{-1}A(\underset{i}{\rightarrow},\underset{j}{\rightarrow})\underset{\alpha}{\rightarrow}=B(\underset{i'}{\rightarrow},\underset{j'}{\rightarrow})\underset{\alpha}{\rightarrow}" title="P^{-1}A(\underset{i}{\rightarrow},\underset{i}{\rightarrow})\underset{\alpha}{\rightarrow}=B(\underset{i'}{\rightarrow},\underset{j'}{\rightarrow})\underset{\alpha}{\rightarrow}" style="display:inline;vertical-align:text-top;">  ②</p>
<p>将①带入②得到：</p>
<p><img src="https://latex.codecogs.com/gif.latex?P^{-1}A=BP^{-1}" title="P^{-1}A=BP^{-1}" style="display:inline;vertical-align:text-top;"></p>
<p>==&gt;</p>
<p><img src="https://latex.codecogs.com/gif.latex?P^{-1}AP=B" title="P^{-1}AP=B" style="display:inline;vertical-align:text-top;"></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/19/matrix-QR-decomposition/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Dorian Zi">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dorian Circle">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/19/matrix-QR-decomposition/" class="post-title-link" itemprop="url">matrix_QR_decomposition</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-03-19 08:52:44 / Modified: 11:24:45" itemprop="dateCreated datePublished" datetime="2019-03-19T08:52:44+08:00">2019-03-19</time>
            

            
              

              
            
          </span>

          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>矩阵的QR分解来源于gram-schmidt正交化. 先看后者</p>
<h1 id="gram-schmidt正交化"><a href="#gram-schmidt正交化" class="headerlink" title="gram-schmidt正交化"></a>gram-schmidt正交化</h1><p>我们有<img src="https://latex.codecogs.com/gif.latex?R^{n}" style="display:inline;vertical-align:text-top;">空间的一组基 <img src="https://latex.codecogs.com/gif.latex?\left&space;(&space;\alpha_{1},\alpha_{1},\alpha_{1},...,\alpha_{n}&space;\right&space;)" style="display:inline;vertical-align:text-top;">，并希望得到该空间的一组正交基<img src="https://latex.codecogs.com/gif.latex?\left&space;(&space;\beta_{1},\beta_{1},\beta_{1},...,\beta_{n}&space;\right&space;)" style="display:inline;vertical-align:text-top;"></p>
<p>1) 令<img src="https://latex.codecogs.com/gif.latex?\beta_{1}=\alpha_{1}" title="\beta_{1}=\alpha_{1}" style="display:inline;vertical-align:text-top;">，则<img src="https://latex.codecogs.com/gif.latex?\alpha_{1}" title="\alpha_{1}" style="display:inline;vertical-align:text-top;">被消灭</p>
<p>2) <img src="https://latex.codecogs.com/gif.latex?\alpha_{2}" title="\alpha_{2}" style="display:inline;vertical-align:text-top;">向<img src="https://latex.codecogs.com/gif.latex?\beta_{1}" title="\beta_{1}" style="display:inline;vertical-align:text-top;">作垂线获得<img src="https://latex.codecogs.com/gif.latex?\beta_{2}" title="\beta_{2}" style="display:inline;vertical-align:text-top;">，那么<img src="https://latex.codecogs.com/gif.latex?\beta_{2}" title="\beta_{2}" style="display:inline;vertical-align:text-top;">和<img src="https://latex.codecogs.com/gif.latex?\beta_{1}" title="\beta_{1}" style="display:inline;vertical-align:text-top;">正交，且可以表示<img src="https://latex.codecogs.com/gif.latex?\alpha_{2}" title="\alpha_{2}" style="display:inline;vertical-align:text-top;">，则<img src="https://latex.codecogs.com/gif.latex?\alpha_{2}" title="\alpha_{2}" style="display:inline;vertical-align:text-top;">被消灭</p>
<p><img src="https://github.com/DorianZi/algorithm_explained/blob/master/res/QR_pic1.png?raw=true" style="display:inline;vertical-align:text-top;"></p>
<p>向量<img src="https://latex.codecogs.com/gif.latex?\alpha_{2}" title="\alpha_{2}" style="display:inline;vertical-align:text-top;">到向量<img src="https://latex.codecogs.com/gif.latex?\beta_{1}" title="\beta_{1}" style="display:inline;vertical-align:text-top;">的正交投影为<img src="https://latex.codecogs.com/gif.latex?\frac{(\alpha_{2},\beta_{1})}{(\beta_{1},\beta_{1})}\beta_{1}" title="\frac{(\alpha_{2},\beta_{1})}{(\beta_{1},\beta_{1})}\beta_{1}" style="display:inline;vertical-align:text-top;"></p>
<p>则<img src="https://latex.codecogs.com/gif.latex?\beta_{2}=\alpha_{2}-&space;\frac{(\alpha_{2},\beta_{1})}{(\beta_{1},\beta_{1})}\beta_{1}" title="\beta_{2}=\alpha_{2}- \frac{(\alpha_{2},\beta_{1})}{(\beta_{1},\beta_{1})}\beta_{1}" style="display:inline;vertical-align:text-top;"></p>
<p>3) <img src="https://latex.codecogs.com/gif.latex?\alpha_{3}" title="\alpha_{3}" style="display:inline;vertical-align:text-top;">向<img src="https://latex.codecogs.com/gif.latex?\beta_{2}" title="\beta_{2}" style="display:inline;vertical-align:text-top;">和<img src="https://latex.codecogs.com/gif.latex?\beta_{1}" title="\beta_{1}" style="display:inline;vertical-align:text-top;">表示的二维平面作垂线，获得<img src="https://latex.codecogs.com/gif.latex?\beta_{3}" title="\beta_{3}" style="display:inline;vertical-align:text-top;">，那么<img src="https://latex.codecogs.com/gif.latex?\beta_{1}" title="\beta_{1}" style="display:inline;vertical-align:text-top;">，<img src="https://latex.codecogs.com/gif.latex?\beta_{2}" title="\beta_{2}" style="display:inline;vertical-align:text-top;">，<img src="https://latex.codecogs.com/gif.latex?\beta_{3}" title="\beta_{3}" style="display:inline;vertical-align:text-top;">互相正交，且可以表示<img src="https://latex.codecogs.com/gif.latex?\alpha_{3}" title="\alpha_{3}" style="display:inline;vertical-align:text-top;">，则<img src="https://latex.codecogs.com/gif.latex?\alpha_{3}" title="\alpha_{3}" style="display:inline;vertical-align:text-top;">被消灭</p>
<p><img src="https://github.com/DorianZi/algorithm_explained/blob/master/res/QR_pic2.png?raw=true" style="display:inline;vertical-align:text-top;"></p>
<p>向量<img src="https://latex.codecogs.com/gif.latex?\alpha_{3}" title="\alpha_{3}" style="display:inline;vertical-align:text-top;">到<img src="https://latex.codecogs.com/gif.latex?\beta_{2}" title="\beta_{2}" style="display:inline;vertical-align:text-top;">和<img src="https://latex.codecogs.com/gif.latex?\beta_{1}" title="\beta_{1}" style="display:inline;vertical-align:text-top;">表示的二维平面的正交投影可以分解为：到<img src="https://latex.codecogs.com/gif.latex?\beta_{2}" title="\beta_{2}" style="display:inline;vertical-align:text-top;">的正交投影，以及到<img src="https://latex.codecogs.com/gif.latex?\beta_{1}" title="\beta_{1}" style="display:inline;vertical-align:text-top;">的正交投影的和</p>
<p>所以正交投影为<img src="https://latex.codecogs.com/gif.latex?\frac{(\alpha_{3},\beta_{1})}{(\beta_{1},\beta_{1})}\beta_{1}&plus;\frac{(\alpha_{3},\beta_{2})}{(\beta_{2},\beta_{2})}\beta_{2}" title="\frac{(\alpha_{3},\beta_{1})}{(\beta_{1},\beta_{1})}\beta_{1}+\frac{(\alpha_{3},\beta_{2})}{(\beta_{2},\beta_{2})}\beta_{2}" style="display:inline;vertical-align:text-top;"></p>
<p>则<img src="https://latex.codecogs.com/gif.latex?\beta_{3}=&space;\alpha_{3}-&space;\left&space;(\frac{(\alpha_{3},\beta_{1})}{(\beta_{1},\beta_{1})}\beta_{1}&plus;\frac{(\alpha_{3},\beta_{2})}{(\beta_{2},\beta_{2})}\beta_{2}&space;\right&space;)" title="\beta_{3}= \alpha_{3}- \left (\frac{(\alpha_{3},\beta_{1})}{(\beta_{1},\beta_{1})}\beta_{1}+\frac{(\alpha_{3},\beta_{2})}{(\beta_{2},\beta_{2})}\beta_{2} \right )" style="display:inline;vertical-align:text-top;"></p>
<p>4) 推广到n维,<img src="https://latex.codecogs.com/gif.latex?\alpha_{n}" title="\alpha_{n}" style="display:inline;vertical-align:text-top;">可以被正交向量<img src="https://latex.codecogs.com/gif.latex?\beta_{n},\beta_{n-1},...,\beta_{1}" title="\beta_{n},\beta_{n-1},...,\beta_{1}" style="display:inline;vertical-align:text-top;">表示，则<img src="https://latex.codecogs.com/gif.latex?\alpha_{n}" title="\alpha_{n}" style="display:inline;vertical-align:text-top;">被消灭，且：</p>
<p><img src="https://latex.codecogs.com/gif.latex?\beta_{n}=\alpha_{n}-\sum_{m=1}^{n-1}\frac{(\alpha_{n},\beta_{m})}{(\beta_{m},\beta_{m})}\beta_{m}" title="\beta_{n}=\alpha_{n}-\sum_{m=1}^{n-1}\frac{(\alpha_{n},\beta_{m})}{(\beta_{m},\beta_{m})}\beta_{m}" style="display:inline;vertical-align:text-top;"></p>
<p>以上即为gram-schmidt正交化。</p>
<h1 id="QR分解"><a href="#QR分解" class="headerlink" title="QR分解"></a>QR分解</h1><p>在前面的gram-schmidt推导过程可知：</p>
<p><img src="https://latex.codecogs.com/gif.latex?\alpha_{1}" title="\alpha_{1}" style="display:inline;vertical-align:text-top;">可以由<img src="https://latex.codecogs.com/gif.latex?\beta_{1}" title="\beta_{1}" style="display:inline;vertical-align:text-top;">表示：<img src="https://latex.codecogs.com/gif.latex?\alpha_{1}=k_{11}\beta{1}" title="\alpha_{1}=k_{11}\beta{1}" style="display:inline;vertical-align:text-top;"></p>
<p><img src="https://latex.codecogs.com/gif.latex?\alpha_{2}" title="\alpha_{2}" style="display:inline;vertical-align:text-top;">可以由正交向量<img src="https://latex.codecogs.com/gif.latex?\beta_{1}" title="\beta_{1}" style="display:inline;vertical-align:text-top;">，<img src="https://latex.codecogs.com/gif.latex?\beta_{2}" title="\beta_{2}" style="display:inline;vertical-align:text-top;">表示：<img src="https://latex.codecogs.com/gif.latex?\alpha_{2}=k_{12}\beta{1}&plus;k_{22}\beta_{2}" title="\alpha_{2}=k_{12}\beta{1}+k_{22}\beta_{2}" style="display:inline;vertical-align:text-top;"></p>
<p><img src="https://latex.codecogs.com/gif.latex?\alpha_{3}" title="\alpha_{3}" style="display:inline;vertical-align:text-top;">可以由正交向量<img src="https://latex.codecogs.com/gif.latex?\beta_{1}" title="\beta_{1}" style="display:inline;vertical-align:text-top;">，<img src="https://latex.codecogs.com/gif.latex?\beta_{2}" title="\beta_{2}" style="display:inline;vertical-align:text-top;">，<img src="https://latex.codecogs.com/gif.latex?\beta_{3}" title="\beta_{3}" style="display:inline;vertical-align:text-top;">表示：<img src="https://latex.codecogs.com/gif.latex?\alpha_{3}=k_{13}\beta{1}&plus;k_{23}\beta_{2}&plus;k_{33}\beta_{3}" title="\alpha_{3}=k_{13}\beta{1}+k_{23}\beta_{2}+k_{33}\beta_{3}" style="display:inline;vertical-align:text-top;"></p>
<p>……</p>
<p><img src="https://latex.codecogs.com/gif.latex?\alpha_{n}" title="\alpha_{n}" style="display:inline;vertical-align:text-top;">可以由正交向量<img src="https://latex.codecogs.com/gif.latex?\left&space;(&space;\beta_{1},\beta_{2},\beta_{3},...,\beta_{n}&space;\right&space;)" title="\left ( \beta_{1},\beta_{2},\beta_{3},...,\beta_{n} \right )" style="display:inline;vertical-align:text-top;">表示：<img src="https://latex.codecogs.com/gif.latex?\alpha_{n}=k_{1n}\beta{1}&plus;k_{2n}\beta_{2}&plus;k_{3n}\beta_{3}&plus;...&plus;k_{nn}\beta_{n}" title="\alpha_{n}=k_{1n}\beta{1}+k_{2n}\beta_{2}+k_{3n}\beta_{3}+...+k_{nn}\beta_{n}" style="display:inline;vertical-align:text-top;"></p>
<p>写成矩阵乘法格式A=QR：</p>
<p><img src="https://latex.codecogs.com/gif.latex?(&space;\alpha_{1},\alpha_{2},\alpha_{3},...,\alpha_{n}&space;)=&space;(\beta_{1},\beta_{2},\beta_{3},...,\beta_{n})&space;\begin{bmatrix}&space;k_{11}&space;&&space;k_{12}&space;&&space;k_{13}&space;&&space;...&space;&&space;k_{1n}\\&space;0&space;&&space;k_{22}&space;&&space;k_{23}&space;&&space;...&space;&&space;k_{2n}\\&space;0&space;&&space;0&space;&&space;k_{33}&space;&&space;...&space;&&space;k_{3n}\\&space;...&space;&&space;...&space;&&space;...&space;&&space;...&space;&&space;...\\&space;0&space;&&space;0&space;&&space;0&space;&&space;...&space;&&space;k_{nn}&space;\end{bmatrix}\right" title="( \alpha_{1},\alpha_{2},\alpha_{3},...,\alpha_{n} )= (\beta_{1},\beta_{2},\beta_{3},...,\beta_{n}) \begin{bmatrix} k_{11} & k_{12} & k_{13} & ... & k_{1n}\\ 0 & k_{22} & k_{23} & ... & k_{2n}\\ 0 & 0 & k_{33} & ... & k_{3n}\\ ... & ... & ... & ... & ...\\ 0 & 0 & 0 & ... & k_{nn} \end{bmatrix}\right" style="display:inline;vertical-align:text-top;"></p>
<p>以上即为A=QR分解。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Dorian Zi</p>
              <div class="site-description motion-element" itemprop="description"></div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">5</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              

              
            </nav>
          

          

          

          

          

          
          

          
            
          
          

        </div>
      </div>

      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Dorian Zi</span>

  

  
</div>


  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v7.0.1</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/src/utils.js?v=7.0.1"></script>

  <script src="/js/src/motion.js?v=7.0.1"></script>



  
  


  <script src="/js/src/schemes/muse.js?v=7.0.1"></script>



  

  


  <script src="/js/src/next-boot.js?v=7.0.1"></script>


  

  

  

  



  




  

  

  

  

  

  

  

  

  

  

  

  

  

  

</body>
</html>
